---
title: "Stat 360 Topic Modeling"
output: html_document
---

## Loading Packages

```{r packages, message=FALSE}
library(tidyverse)
library(tm)
library(stringr)
library(stringi)
library(wordcloud)
library(topicmodels)
library(ldatuning)
library(usethis)
library(LDAvis)
library(servr)
```

## Loading Data

```{r loading, message=FALSE}
debates <- read_csv("debate_transcripts_v3_2020-02-26.csv")
```

## Cleaning Data

```{r data}
# Set global seed
set.seed(1)

# Here I drop all the columns except speaker and speech

keeps <- c("speaker", "speech")
debates<- debates[ , keeps, drop = FALSE]

# A list of all the democratic candidates that appeared in any of the debates -- searched online

demCandidates <- c("Michael Bennet", "Joe Biden", "Cory Booker", "Pete Buttigieg", "Julian Castro", "Bill de Blasio", "John Delaney", "Tulsi Gabbard", "Kirsten Gillibrand", "Kamala Harris", "Jay Inslee", "Amy Klobuchar", "Beto O'Rourke", "Tim Ryan", "Bernie Sanders", "Eric Swalwell", "Elizabeth Warren", "Marianne Williamson", "Andrew Yang", "Michael Bloomberg")

# Filtering the rows out that are not spoken by one of the democratic candidates. Also lol it took me like half an hour to figure out how to include Beto since he had a special symbol in his name

debates <- subset(debates, speaker %in% demCandidates)

# Double checking we got everyone
length(demCandidates)
length(unique(debates$speaker))

# Collapsing the speech text by speaker, such that each candidates total responses from all of the debates forms a document

debates <- aggregate(speech ~ speaker, data = debates, FUN = paste, collapse = " ")

## NOTE: I think I did this correctly with the collapse parameter, however I'm not sure if the speech from a candidate pasted all the quotes together WITH or WITHOUT a space which is important

## Next I am removing punctuation from the speeches

debates$speech <- removePunctuation(debates$speech)
```

## Start using the tm package here for some fancy shenanigans

```{r tmMagic}
# definining a "corpus" from the dataframe
debates_source <- VectorSource(debates$speech)
debates_corpus <- VCorpus(debates_source)

# stripping whitespace
debates_corpus <- tm_map(debates_corpus, stripWhitespace)

# converting to all lower case
debates_corpus <- tm_map(debates_corpus, content_transformer(tolower))

# removing common stopwords aka words such as 'a', 'the', 'also'
debates_corpus <- tm_map(debates_corpus, removeWords, stopwords("english"))

# removing numbers
debates_corpus <- tm_map(debates_corpus, removeNumbers)

# removing punctuation
debates_corpus <- tm_map(debates_corpus, removePunctuation)

#removing other uninformative words that came up

Stopwords <- c("can", "actually", "weve", "also", "whats", "youre", "think", "tell", "yes", "want", "well", "bit", "get", "thats", "theyre", "thing")
debates_corpus <- tm_map(debates_corpus, removeWords, Stopwords)
```

## Moving on to creating matrices of data

```{r matrixMagic}
#created a matrix with speaker aka document as the rows (1 - 20) and word as column, with first column being the most frequent word used in the total documents. A ij of the matrix represents that the word in the column index was spoken j times by the ith candidate
dtm <- DocumentTermMatrix(debates_corpus)
inspect(dtm)

# list of words spoken by candidates at least 100 total times during the debate
findFreqTerms(dtm, 100)
```

## Visualizing data as wordclouds

```{r wordcloud}
set.seed(1)

# wordcloud of all the candidates together

freq <- colSums(as.matrix(dtm)) 
wordcloud(names(freq), freq, min.freq=150, colors=brewer.pal(6, "Dark2"))  
```

## Visualizing each candidate's wordcloud

```{r personal-wordclouds}
# for(i in 1:20){
#   m <- as.matrix(dtm[i,])
#   freq <- colSums(m) 
#   wordcloud(names(freq), m, min.freq=50, colors=brewer.pal(6, "Dark2"))  
# }
```

## Example LDA Model

This LDA function is from the `topicmodels` package. It "estimate[s] a LDA model using for example...Gibbs Sampling" [https://cran.r-project.org/web/packages/topicmodels/topicmodels.pdf]. This will return an object of class LDA, on which we can perform analysis.

```{r model}
ALPHA <- .1 ; K <- 7 ; S <- 2000 ; BURNIN = 0

lda <- LDA(dtm                              #inputting our document-term-matrix
           , method = "Gibbs"               #we will be using Gibbs sampling
           , control = list(alpha = ALPHA   #inputting alpha hyperparameter
           , seed = 1                       #setting seed to 1
           , delta = .1                     #setting delta hyperparameter
           , iter = S                       #number of trials equal to S
           , burnin = BURNIN)               #adding burn in period
           , k = K)                         #fixing the number of topics
```

These two functions return a list or matrix containing the most likely terms for each topic and the most likely topics for each document, respectively:

```{r visualizing}
topics(lda, k = 7)
terms(lda, k = 7)
```

This function comes from the `ldatuning` package. This function "calculates different metrics to estimate the most preferable number of topics for LDA model" [https://cran.r-project.org/web/packages/ldatuning/ldatuning.pdf]. Here, it uses 3 scoring algorithsm to evaluate the number of topics: Arun 2010, CaoJuan 2009, and Griffiths 2004. 

```{r optimalTopics}
topicStats <- FindTopicsNumber(dtm
                ,topics = seq(3, 30, by = 1)
                ,metrics = c("Arun2010", "CaoJuan2009", "Griffiths2004")
                ,method = "Gibbs"
                ,control = list(alpha = ALPHA   #inputting alpha hyperparameter
                ,seed = 1                       #setting seed to 1
                ,delta = .1                     #setting delta hyperparameter
                ,iter = S                       #number of trials equal to S
                ,burnin = BURNIN))              #adding burn in period
```

This function takes the output from the previous function, a number of topics and the corresponding value from the given metric, and plots the change in values over the number of topics for each metric:

```{r plotting-model-criterion}
FindTopicsNumber_plot(topicStats)
```

After running producting the lda model above, the posterior function helps to evaluate the posterior probabilities of the specific terms per each topic and the specific topics per document. 

```{r posterior-lda}
posterior(lda)
```

Question: Do we want to consider perplexity? Split the data into training and testing to see how well the model predicts for different candidates?

## Visualize fitted model with LDAvis

Function from [https://gist.github.com/trinker/477d7ae65ff6ca73cace] to connect the output from the LDA function in the `topicmodels` package and the visualization tools in the `LDAvis` package:

```{r create-json}
topicmodels2LDAvis <- function(x, ...){
    post <- posterior(x)
    if (ncol(post[["topics"]]) < 3) stop("The model must contain > 2 topics")
    mat <- x@wordassignments
    createJSON(
        phi = post[["terms"]], 
        theta = post[["topics"]],
        vocab = colnames(post[["terms"]]),
        doc.length = slam::row_sums(mat, na.rm = TRUE),
        term.frequency = slam::col_sums(mat, na.rm = TRUE)
    )
}

json <- topicmodels2LDAvis(lda)
```

```{r visualize-json}
serVis(json, open.brower=interactive())

# To potentially save the visualization locally and host it, we can use this code
# If we want to publish it on Github, we can add as.gist=TRUE
#serVis(json, out.dir="Desktop", open.browser=interactive())
```
